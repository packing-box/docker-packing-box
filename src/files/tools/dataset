#!/usr/bin/python3
# -*- coding: UTF-8 -*-
from pbox import *
from pbox.__info__ import *
from tinyscript import *


__version__     = "2.5.0"
__doc__         = """
This tool aims to manipulate a dataset in many ways including its creation, enlargement, update, alteration, selection,
 merging, export or purge.
"""
__examples__    = [
    "alter my-dataset --percentage 30",
    "alter my-dataset --query \"label == '-'\"",
    "browse my-dataset --query \"label == 'upx'\"",
    "convert my-dataset --new-name my-fileless-dataset",
    "export my-dataset --format packed-samples --output my-dataset-files -n 100",
    "fix my-dataset --detect",
    "fix my-dataset --labels my-labels.json",
    "ingest dataset-packed-elf --detect",
    "ingest dataset-packed-pe --labels /path/to/exe/labels.json --merge --rename as-is",
    "list --show-all",
    "make my-dataset -f ELF -n 500 --source-dir /path/to/dotnet/exe",
    "make my-dataset -f PE,ELF -n 2000",
    "merge my-dataset dataset-to-be-merged",
    "plot features my-dataset byte_0_after_ep byte_1_after_ep",
    "plot features my-dataset number_resources --format png",
    "plot labels my-dataset",
    "purge all",
    "purge test-*",
    "remove my-dataset --query \"label == 'aspack'\"",
    "select my-dataset my-new-dataset --query \"label == 'upx'\"",
    "show my-dataset --per-format --limit 20",
    "update my-dataset --detect --refresh -s /path1/to/exe -s /path2/to/exe",
    "update my-dataset --source-dir /path/to/exe --labels /path/to/exe/labels.json",
    "view my-dataset --query \"ctime > 2020\"",
    "view my-dataset -q \"'PE32' in format\"",
]
__description__ = "Make datasets of packed and not packed executables for use with Machine Learning"


def __add_name(parser, force=False, name="name", help="name of the dataset", note=None):
    def dataset_exists(string):
        if string == "all" or "*" in string:
            return string
        p = config['datasets'].joinpath(string)
        (ts.folder_exists_or_create if force else ts.folder_exists)(str(p))
        if not force and not Dataset.check(string) and not FilelessDataset.check(string):
            raise ValueError("Bad dataset")
        return p
    parser.add_argument(name, type=dataset_exists, help=help, **({} if note is None else {'note': note}))
    return parser


def _percentage(p):
    try:
        p = float(p)
        if 0. <= p <= 100.:
            return p / 100.
    except ValueError:
        pass
    return ValueError("Not a percentage")


if __name__ == '__main__':
    cmds = parser.add_subparsers(dest="command", help="command to be executed")
    alter = __add_name(cmds.add_parser("alter", help="alter the target dataset with a set of transformations"))
    alter.add_argument("-a", "--modifiers-set", type=ts.file_exists, default=str(config['modifiers']),
                       help="modifiers set's YAML definition")
    agroup = alter.add_mutually_exclusive_group()
    agroup.add_argument("-p", "--percentage", type=_percentage, default=20, help="percentage of samples to be altered")
    agroup.add_argument("-q", "--query", default="all", help="query for filtering records to be selected",
                        note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    browse = __add_name(cmds.add_parser("browse", help="compute features and browse the resulting data"))
    browse.add_argument("-f", "--features-set", type=ts.file_exists, default=str(config['features']),
                        help="features set's YAML definition")
    browse.add_argument("-q", "--query", default="all", help="query for filtering records to be selected",
                        note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    convert = __add_name(cmds.add_parser("convert", help="convert the target dataset to a fileless one"))
    convert.add_argument("-f", "--features-set", type=ts.file_exists, default=str(config['features']),
                         help="features set's YAML definition")
    convert.add_argument("-n", "--new-name", help="name for the new converted dataset",
                         note="if None, the original dataset is overwritten")
    __add_name(cmds.add_parser("edit", help="edit the data file"))
    export = __add_name(cmds.add_parser("export", help="export packed samples from a dataset or export the dataset"
                                                       " to a given format"))
    export.add_argument("-f", "--format", default="dsff", choices=("arff", "csv", "dsff", "packed-samples"),
                        help="output format")
    export.add_argument("-o", "--output", default="export", metavar="F", type=ts.folder_does_not_exist,
                        help="output folder or file for the export", note="the extension gets added automatically")
    export_ = export.add_argument_group("option when exporting packed samples", before="extra arguments")
    export_.add_argument("-n", "--number-samples", dest="n", type=ts.pos_int, default=0,
                         help="number of packed samples to be exported")
    fix = __add_name(cmds.add_parser("fix", help="fix a corrupted dataset"))
    fgroup = fix.add_mutually_exclusive_group()
    fgroup.add_argument("-d", "--detect", action="store_true", help="detect used packer with installed detectors")
    fgroup.add_argument("-l", "--labels", type=ts.json_config, help="set labels from a JSON file")
    ingest = cmds.add_parser("ingest", help="ingest samples from a folder into new dataset(s)")
    ingest.add_argument("folder", type=ts.folder_exists, help="target folder with subfolders containing samples")
    igroup = ingest.add_mutually_exclusive_group()
    igroup.add_argument("-d", "--detect", action="store_true", help="detect used packer with installed detectors")
    igroup.add_argument("-l", "--labels", type=ts.json_config, help="set labels from a JSON file")
    ingest.add_argument("--merge", action="store_true", help="merge all subfolders into a single dataset")
    ingest.add_argument("-m", "--min-samples", type=ts.pos_int, default=100,
                        help="minimum of samples to be found in subfolders for being kept")
    ingest.add_argument("-M", "--max-samples", type=ts.pos_int, default=0,
                        help="maximum of samples to be found in subfolders for being kept")
    ingest.add_argument("-p", "--prefix", default="", help="prefix to be added to the name(s) of the new dataset(s)")
    ingest.add_argument("-r", "--rename", default="slugify", choices=tuple(RENAME_FUNCTIONS.keys()),
                        help="apply a function for naming the new dataset(s)")
    ingest.add_argument("-x", "--exclude", nargs="*", help="folder to be excluded")
    listds = cmds.add_parser("list", help="list all the datasets from the workspace")
    listds.add_argument("-a", "--show-all", action="store_true", help="show all datasets even those that are corrupted")
    listds.add_argument("-f", "--hide-files", action="store_true", help="hide the 'files' column")
    listds.add_argument("-r", "--raw", action="store_true", help="display unformatted text", note="useful with grep")
    make = __add_name(cmds.add_parser("make", help="add n randomly chosen executables from input sources to the "
                                                   "dataset"), True)
    make_ = make.add_mutually_exclusive_group()
    make_.add_argument("-a", "--pack-all", action="store_true", help="pack all executables",
                       note="this cannot be set with -b/--balance")
    make_.add_argument("-b", "--balance", action="store_true", help="balance the dataset relatively to the number of "
                                                                    "packers used, not between packed and not packed")
    make.add_argument("-f", "--formats", type=ts.values_list, default="All", help="list of formats to be considered")
    make.add_argument("-n", "--number", dest="n", type=ts.pos_int, default=100,
                      help="number of executables for the output dataset")
    make.add_argument("-p", "--packer", action="extend", nargs="*", type=lambda p: Packer.get(p),
                      help="packer to be used")
    make.add_argument("-s", "--source-dir", action="extend", nargs="*", type=lambda p: ts.Path(p, expand=True),
                      help="executables source directory to be included")
    merge = __add_name(cmds.add_parser("merge", help="merge two datasets"))
    __add_name(merge, name="name2", help="name of the dataset to merge")
    merge.add_argument("-n", "--new-name", help="name for the new merged dataset",
                       note="if None, the original dataset is overwritten")
    plot = cmds.add_parser("plot", help="plot something about the dataset")
    pcmds = plot.add_subparsers(dest="subcommand", help="command to be executed")
    plot_feat = __add_name(pcmds.add_parser("features", help="plot the distribution of a given feature or multiple "
                                                             "features combined (bar chart)"))
    plot_feat.add_argument("feature", action="extend", nargs="*", help="feature identifiers")
    plot_feat.add_argument("-f", "--format", choices=("jpg", "png", "tif", "svg"), default="png",
                           help="image file format for plotting", note="no format means plotting in the terminal")
    plot_feat.add_argument("-m", "--multiclass", action="store_true",
                           help="process features using multiple label classes")
    plot_lab = __add_name(pcmds.add_parser("labels", help="plot the distribution of labels in the dataset (pie chart)"))
    plot_lab.add_argument("-f", "--format", choices=("jpg", "png", "tif", "svg"), default="png",
                          help="image file format for plotting")
    purge = __add_name(cmds.add_parser("purge", help="purge a dataset"), True,
                       note="use 'all' to purge every dataset or the wildcard '*' to select a part of them")
    purge.add_argument("-b", "--backup", action="store_true", help="only purge backups")
    remove = __add_name(cmds.add_parser("remove", help="remove executables from a dataset"))
    remove.add_argument("-q", "--query", default="all", help="query for filtering records to be removed",
                        note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    rename = __add_name(cmds.add_parser("rename", help="rename a dataset"))
    rename.add_argument("name2", type=ts.folder_does_not_exist, help="new name of the dataset")
    __add_name(cmds.add_parser("revert", help="revert a dataset to its previous state"))
    select = __add_name(cmds.add_parser("select", help="select a subset of the dataset"))
    select.add_argument("name2", type=ts.folder_does_not_exist, help="name of the new dataset")
    select.add_argument("-n", "--number", dest="limit", type=ts.pos_int, default=0,
                        help="limit number of executables for the output dataset", note="0 means all")
    select.add_argument("-q", "--query", default="all", help="query for filtering records to be selected",
                        note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    show = __add_name(cmds.add_parser("show", help="get an overview of the dataset"))
    show.add_argument("-l", "--limit", default=10, type=int, help="number of executables to be displayed per format")
    show.add_argument("--per-format", action="store_true", help="display statistics per format")
    update = __add_name(cmds.add_parser("update", help="update a dataset with new executables"), True)
    ugroup = update.add_mutually_exclusive_group()
    ugroup.add_argument("-d", "--detect", action="store_true", help="detect used packer with installed detectors")
    ugroup.add_argument("-l", "--labels", type=ts.json_config, help="set labels from a JSON file")
    update.add_argument("-n", "--number", dest="n", type=ts.pos_int, default=0,
                        help="number of executables for the output dataset", note="0 means all")
    update.add_argument("-r", "--refresh", action="store_true", help="refresh labels of already existing executables")
    update.add_argument("-s", "--source-dir", action="extend", nargs="*", type=lambda p: ts.Path(p, expand=True),
                        help="executables source directory to be included")
    view = __add_name(cmds.add_parser("view", help="view executables filtered from a dataset"))
    view.add_argument("-q", "--query", default="all", help="query for filtering records to be viewed",
                      note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    initialize(noargs_action="help")
    logger.name = "dataset"
    logging.configLogger(logger, ["INFO", "DEBUG"][args.verbose], fmt=LOG_FORMATS[args.verbose], relative=True)
    name, args.load = getattr(args, "name", None), args.command not in ["ingest", "list", "purge"]
    if args.command == "purge" and isinstance(name, str):
        purged = False
        if name == "all":
            for ds in Dataset.iteritems(True):
                ds.purge()
                purged = True
        elif "*" in name:
            name = r"^%s$" % name.replace("*", "(.*)")
            for ds in Dataset.iteritems(False):
                if re.search(name, ds.stem):
                    open_dataset(ds).purge()
                    purged = True
        if not purged:
            logger.warning("No dataset to purge in workspace (%s)" % str(config['datasets']))
    else:
        if args.command == "ingest":
            args.rename_func = RENAME_FUNCTIONS[args.rename]
        if hasattr(args, "modifiers_set"):
            Modifiers.source = args.modifiers_set  # configure the modifiers set's YAML definition source
        if hasattr(args, "features_set"):
            Features.source = args.features_set  # configure the features set's YAML definition source
        ds = (FilelessDataset if getattr(args, "fileless", args.command == "list") or \
              name and FilelessDataset.check(name) else Dataset)(**vars(args))
        getattr(ds, args.command)(**vars(args))
        # it may occur that packing fails with a silenced error and that the related executable file remains in the
        #  files subfolder of the dataset while not handled in data.csv, hence creating an inconsistency ;
        #  fixing the dataset right after the make command allows to avoid this inconsistency
        if args.command == "make":
            ds.fix()

